# 神经网络——基本理论文档

### 1.基本原理

##### 1.1 生物认知与神经网络

​	传统的计算机按照严格的串行顺序，相当准确具体地处理数据，不存在模糊性或不确定性。而另一方面，动物的大脑表面上看起来以慢得多的节奏运行，却似乎以并行方式处理信号，模糊性是其计算的一种特征。 

​					<img src="https://i.loli.net/2021/03/21/5IDhsfT3i9t6Vuk.png" alt="截屏2021-03-21 下午2.22.08" style="zoom: 50%;" />                         

脑神经科学中互相连接的神经元组成的生物大脑便是人工神经网络（ANN）的灵感来源。



<img src="https://i.loli.net/2021/04/03/nWAhGbzpQYsZiOM.png" style="zoom: 50%;" />

​	探究生物大脑中的基本单元——神经元的工作流程。基本上所有的神经元都是将电信号从一端传输 到另一端，沿着轴突，将电信号从树突传到树突。然后，这些信号从一个神经元传递到另一个神经元。

<img src="https://i.loli.net/2021/04/03/eNkFXwC1SV3jWxc.png" style="zoom: 67%;" />

​	这就是身体感知光、声、触压、热等信号的机制。来自专门的感觉神经元的信号沿着神经系统，传输到大脑，而大脑本身主要也是由神经元构成的。

​	同时，神经元在受到刺激后并不会立即反应，而是先抑制输入，直到输入增强， 强大到可以触发输出，也就是说——**神经元不希望传递微小的噪声信号，而只是传递有意识的明显信号**。

​	只有输入超过了阈值（threshold）， 足够接通电路，才会产生输出信号。

<img src="https://i.loli.net/2021/04/03/T15sCOHq8jZ7np9.png" style="zoom:50%;" />

所以我们还要将某种称为**激活函数**的阈值考虑在内。比如下面展示的一个最常用的激活函数——为**S函数（sigmoid function）**，有时也叫作逻辑函数。

<img src="https://i.loli.net/2021/04/03/mfX5RjYUV3O2CQT.png" alt="截屏2021-03-21 下午2.51.57" style="zoom:50%;" />

​	大多数人工神经网络会使用S形函数还有一个非常重要的原因，就是S函数比起其他S形函数对于计算机计算起来要容易得多。

当然，还有一件事情是生物神经元可以**接受许多输入**。在实现过程中，我们通常只需要对它们进行相加，得到最终总和，作为S函数的输入，然后输出结果。

<img src="https://i.loli.net/2021/04/03/Fuhg3HfvEGyQV1Y.png" alt="截屏2021-03-21 下午2.59.40" style="zoom: 50%;" />

如果组合信号不够强大，那么S阈值函数的效果是抑制输出信号。而只要总和x足够大，S函数的效果就是激发神经元。

​	继续观察其运作原理，假设如果只有其中一个输入足够大，其他输入都很小，那么这也足够激发神经元。更重要的是，如果其中一些输入，单个而言一般大，但不是非常大，这样由于信号 的组合足够大，超过阈值，那么神经元也能激发。

​	对比生物神经网络结构，树突收集了这些电信号，将其组合形成更强的电信号。如果信号足够强，超过阈值，神经元就会发射信号，沿着轴突，到达终端，将信号传递 给下一个神经元的树突。

所以，这样的结构设计就给人一种直观上的感觉，即**这些神经元也可以进行一些相对复杂、在某种意义上有点模糊的计算**。 

人工神经网络具有生物系统一些显著特点，如非线性、高度并行、鲁棒性、容错性、学习能力以及处理不精确和模糊的信息。它提供了一种从样例中学习离散、连续或者向量形式的函数实用方法。

##### 1.2 人工神经网络的基本结构

​	将上述这种自然形式复制到人造模型的一种方法是，构建多层神经元，每 一层中的神经元都与在其前后层的神经元互相连接。

<img src="https://i.loli.net/2021/04/03/RXwx5hrIB6LpMgC.png" style="zoom: 50%;" />

这是一个三层神经元，每一层有三个人工神经元或节点。

针对训练样本，应该如何调整做出反应来实现学习过程呢？最明显的一点就是**调整节点之间的连接强度**。

<img src="https://i.loli.net/2021/04/03/Dx7FTI4hp6L9ywi.png" alt="截屏2021-03-21 下午3.14.18" style="zoom:53%;" />

如上图，一个最基本的神经网络由三层构成：

* 第1层节点是**输入层**。这一层不做其他事情，仅表示输入信号。也就是说，输入节点不对输入值应用激活函数。这层所 

  做的所有事情就是表示输入，仅此而已。 

* 第2层节点是**隐藏层**。这一层的每个节点，我们需要算出组合输入。

<img src="https://i.loli.net/2021/04/03/TXDiIZMBhV6ftcR.png" style="zoom: 50%;" />

​		这一计算过程可以用一个非常简洁的公式来表示：
$$
X=W·I
$$
​	    其中，$W$是权值矩阵，$I$是输入矩阵，$X$是组合调节后的信号。

​		接着，将调节后的信号代入激活函数(这里用的是S函数)中：
$$
O = sigmoid ( X )
$$
​		$O$ 代表矩阵，这个矩阵包含了来自神经网络的最后一层中的所有输出。

* 第3层节点是**输出层**。使用第2层的输出作为第3层的输入，再次进行矩阵乘法。当然，这个输出应该使用权重系数进行调节并进行组合。 



采用这样的结构设计主要原因有以下两点：

* 第一是这种一致的完全连接形式事实上可以相对容易地编码成计算机指令；
* 第二是神经网络的学习过程将会弱化这些实际上不需要的连接（也就是这些连接的权重将趋近于0），因此对于解决特定任务所需最小数量的连接冗余几个连接，也无伤大雅。

这意味着，随着神经网络学习过程的进行，神经网络通过调整优化网络内部的链接权重改进输出，一些权重可能会变为零或接近于零。

​	零或几乎为零的权重意味着这些链接对网络的贡献为零，因为没有传递信号。零权重意味着信号乘以零，结果得到零，因此这个链接实际上是被断开了。 



##### 1.3 反向传播与参数优化

<img src="https://i.loli.net/2021/04/03/62HT7o4LI5tUBu1.png" style="zoom:50%;" />

​	在经过上述三层结构的一轮处理后，为来实现“学习”的过程，我们需要将神经网络的输出值与训练样本中的输出值进行比较，计算出误差。这里需要使用这个误差值来调整神经网络本身，进而改进神经网络的输出值。



**反向传播：**

​	那么神经网络如何根据误差来更新链接权重呢？ 

<img src="https://i.loli.net/2021/04/03/1D4LgWGNkmv9ZR2.png" style="zoom:50%;" />

主要有以下两种思想：

​	一种思想就是在所有造成误差的节点中平分误差。

<img src="https://i.loli.net/2021/04/03/LvgVYKs9ZNrCxod.png" style="zoom:50%;" />

相对的，另一种种思想是不等分误差。

<img src="https://i.loli.net/2021/04/03/CeRNIw82oF5VuyJ.png" style="zoom:50%;" />

我们为较大链接权重的连接分配更多的误差，这是因为这些链接对造成误差的贡献较大。

​	将同样的思想扩展到多个节点。如果我们拥有100个节点链接到输出节点，那么我们要在这100条链接之间，按照每条链接对误差所做贡献的比例（由链接权重的大小表示），分割误差。 



在神经网络整个运作过程中，只在两件事情上使用了权重：

* 第一件事情，在神经网络中，我们使用权重，将信号从输入向前传播到输出层。

* 第二件事情，我们使用权重，将误差从输出向后传播到网络中。我们称这种方法为自己**反向传播**。



为来更具体说明反向传播，这里以一个具有2个输入节点和2个输出节点的简单网络为例。

<img src="https://i.loli.net/2021/04/03/ZKfCAUFEO24GX3q.png" style="zoom:50%;" />

两个输出节点都有误差，所以在网络中，我们需要使用这两个误差值来告知如何调整内部链接权重。

​	我们将第一个输出节点的误差标记为$e_1$ ,这个值等于由训练数据提供的所期望的输出值$t_1$ 与实际输出值$o_1$之间的差。
$$
e_1=( t_1 -o_1 )
$$
​	同样，将第二个输出节点的误差标记为$e_2$。

​	然后，按照所连接链接的比例，也就是权重$w _{1,1}$ 和$w _{2,1}$，我们对误差$e_1$进行了分割。类似地，我们按照权重$w _{1,2}$  和$w _{2,2}$  的比例分割了$e_2$ 。

<img src="https://i.loli.net/2021/04/03/1kFDVSJdATpwYEi.png" alt="截屏2021-03-21 下午4.22.08" style="zoom:50%;" />

​	也就是误差e1 要分割更大的值给较大的权重，分割较小的值给较小的权重。



在明确了如何使用误差来指导在网络内部调整链接权重的原理后，我们还要考虑如何将这样的反向传播误差到更多层中 。

​	接着以一个3层的简单神经网络为例。

<img src="https://i.loli.net/2021/04/03/4oHjMnrFg1iauO8.png" style="zoom:50%;" />

​	从右手边的最终输出层开始，往回工作，我们可以看到，我们使用在输出层的误差值引导调整馈送到最终层的链接权重。更一般地，我们将输出误差标记为$e_{output}$ ，将在输出层和隐藏层之间的链接权重标记为$w_{ho}$ 。通过将误差值按权重的比例进行分割，我们计算出与每条链接相关的特定误差值。

​	对于额外的新层所需要做的事情，则继续采用与隐藏层节点相关联的这些误差$e_{hidden}$ ，再次将这些误差按照输入层和隐藏层之间的链接权重$w_{ih}$ 进行分割。

<img src="/Users/suneann/Library/Application Support/typora-user-images/截屏2021-03-21 下午4.33.36.png" style="zoom:50%;" />

​	如果神经网络具有多个层，那么我们就从最终输出层往回工作，对每一层重复应用相同的思路。

当然，这样的工作结构也带来率额外的问题，对 于隐藏层节点$e_{hidden}$ ，该如何使用什么误差呢？

​	对于隐藏层的节点来说，我们没有目标值或所希望的输出值，我们只有最终输出层节点的目标值，这个目标值来自于训练样本数据。

​	所以，我们可以换种思路考虑这个问题，隐藏层第一个节点具有两个链接，这两个链接将这个节点连接到两个输出层节点。我们知道，沿着各个链接可以分割输出误差。这意味着，对于中间层节点的每个链接，我们得到了某种误差值。

<img src="https://i.loli.net/2021/04/03/8hO54Bq1vTmAeZy.png" alt="截屏2021-03-21 下午4.40.21" style="zoom:50%;" />

也就是说，可以**重组这两个链接的误差，形成这个节点的误差**。

​	因此，第一个隐藏层节点的误差是与这个节点前向连接所有链接中分割误差的和
$$
e_{hidden,1}=链接w_{1,1}和链接w_{1,2}上分割误差之和=e_{output,1}*\frac{w_{1,1}}{w_{1,1}+w_{2,1}} +e_{output,2}*\frac{w_{1,2}}{w_{1,2}+w_{2,2}}
$$
<img src="https://i.loli.net/2021/04/03/MqtDCxky6cK1mWr.png" style="zoom:50%;" />

同样的，更进一步向后工作，在前一层中应用相同的思路。

<img src="https://i.loli.net/2021/04/03/ISHrdgWcmVRsf5G.png" style="zoom:50%;" />

**参数优化：**

​	我们还没有解决在神经网络更新链接权重中非常核心的问题——**如何根据误差来更新权重**。

​	数学太复杂了，因此我们不能使用微妙的代数直接计算出的权重。当我们通过网络前馈信号时，有太多的权重需要组合，太多的函数的函数的函数……需要组合。想想看，即使是一个只有3层、每层3个神经元的小小的神经网络，就像我们刚才使用的神经网络，也具有太多的权重和函数需要组合。

​	在此情况下，你如何调整输入层第一个节点和隐藏层第二个节点之间链路的权重，以使得输出层第三个节点的输出增加0.5呢？即使我们碰运气做到了这一点，这个效果也会由于需要调整另一个权重来改进不同的输出节点而被破坏。

可以观察下面这一表达式，一个简单的3层、每层3个节点的神经网络，其中输入层节点的输出是输入值和链接权重的函数。
$$
o_k=\frac{1}{1+e^{-\sum^{3}_{j=1} \left( w_{j,k}.\frac{1}{1+e^{-\sum^{3}_{i=1} \left( w_{i,j}.x_j\right)  }} \right)}}
$$
​	现在的目标是找到好的权重组合，由上边的式子可以看出，采用暴力计算的手段是不可取的。 

于是，**梯度下降**的方法就这样提出了。

​	梯度下降法给我们带来一种能力，即我们不必完全理解复杂的函数，而是使用更小的步子朝着实际的最小值方向迈进，优化答案，直到我们对于所得到的精度感到满意为止。而这个调节步子大小的参数我们称之为**学习率**。

​	整个过程由下面这一公式来表示（详细过程之后再补充）。

<img src="https://i.loli.net/2021/04/03/b3VOv28WPTpugSt.png" alt="截屏2021-04-03 下午5.01.04" style="zoom: 33%;" />

### 2.数据准备

##### 2.1 输入数据

​	由S激活函数可以发现，如果输入变大，激活函数就会变得非常平坦。 

<img src="https://i.loli.net/2021/04/03/TSKZ16NYXOvinmD.png" alt="截屏2021-03-21 下午5.24.25" style="zoom:50%;" />

如果使用梯度学习新的权重，一个平坦的激活函数会出带来很大问题。

​	回头仔细观察关于权重变化的表达式。权重的改变取决于激活函数的梯度。小梯度意味着限制神经网络学习的能力。这就是所谓的饱和神经网络。这意味着，我们应该**尽量保持小的输入**。 

​	一个好的建议是重新调整输入值，将其范围控制在0.0到1.0。当然，输入0会将$o_j$ 设置为0，这样权重更新表达式就会等于0，从而造成学习能力的丧失，因此在某些情况下，我们会将此输入加上一个小小的偏移，如0.01，避免输入0带来麻烦。 

##### 2.2 输出数据

​	神经网络的输出是最后一层节点弹出的信号。如果我们使用的激活函数不能生成大于1的值，那么尝试将训练目标值设置为比较大的值就有点愚蠢了（逻辑函数不能取到1.0，只能接近1.0）。

<img src="https://i.loli.net/2021/04/03/eWtkSyhVRu9fDrY.png" style="zoom:45%;" />

​	如果我们将目标值设置在这些不可能达到的范围，训练网络将会驱使更大的权重，以获得越来越大的输出，而这些输出实际上是不可能由激活函数生成的，这会使得网络饱和，

​	因此，我们还需要重新调整目标值，匹配激活函数的可能输出，注意避开激活函数不可能达到的值。 

​	常见的使用范围为0.0～1.0，但是由于0.0和1.0这两个数也不可能是目标值，并且有驱动产生过大的权重的风险，因此一些人也使用0.01 ～0.99的范围。 

##### 2.3 随机初始权重

​	与输入和输出一样，同样的道理也适用于初始权重的设置。由于大的初始权重会造成大的信号传递给激活函数，导致网络饱和，从而降低网络学习到更好的权重的能力，因此应该避免大的初始权重值。 

​	可以从-1.0～+1.0之间随机均匀地选择初始权重。

​	在此处不纠结于计算细节，但是，其核心思想是，如果很多信号进入一个节点（这也是在神经网络中出现的情况），并且这些信号的表 

现已经不错了，不会太大，也不会分布得奇奇怪怪，那么在对这些信号进行组合并应用激活函数时，权重应该支持保持这些表现良好的信号。

​	直觉上说，这是有意义的。一些过大的初始权重将会在偏置方向上偏置激活函数，非常大的权重将会使激活函数饱和。一个节点的传入链接越多，就有越多的信号被叠加在一起。因此，如果链接更多，那么减小权重的范围，这个经验法则是有道理的。 

<img src="https://i.loli.net/2021/04/03/t74wTAypDkOCrus.png" style="zoom:45%;" />

​	从概率分布中进行采样的思想，那么这一经验法则实际上讲的是，从均值为0、标准方差等于节点传入链接数量平方根倒数的正态分布中进行采样。但是，由于经验法则所假设的一些事情，如可替代的激活函数tanh()、输入信号的特定分布等，可能不是真的，因此，我们不必太担心要精确正确地理解这个法则。 

##### 2.4 总结

在这儿将上诉内容整理一下：

* 输入应该调整到较小值，但不能为零。一个常见的范围为0.01～0.99，或-1.0～1.0，使用哪个范围，取决于是否匹配了问题。
* 输出应该在激活函数能够生成的值的范围内。逻辑S函数是不可能生成小于等于0或大于等于1的值。将训练目标值设置在有效的范围之外，将会驱使产生越来越大的权重，导致网络饱和。一个合适的范围为0.01～0.99。

### 3.编程部分

##### 3.1 框架代码

```python
# neural network class definition
class neuralNetwork : 
    # initialise the neural network 
    def __init__() : 
        pass
    # train the neural network 
    def train() : 
        pass
    # query the neural network 
    def query() :
        pass
```

##### 3.2 初始化网络

```python
# initialise the neural network 
    def __init__(self,inputnodes,hiddennodes,outputnodes,learningrate): 
        # set number of nodes in each input, hidden, output layer 
        self . inodes = inputnodes 
        self . hnodes_1 = hiddennodes_1 
        self . hnodes_2 = hiddennodes_2
        self . onodes = outputnodes 
        # learning rate 
        self . lr = learningrate 
        pass
```

​	定义的神经网络类，输入层创建7个节点、隐藏层2个， 输出层1个，学习率为0.5的小型神经网络对象。 

```python
# number of input, hidden and output nodes 
input_nodes = 7 
hidden_nodes_1 = 15
hidden_nodes_2 = 5 
output_nodes = 1 
# learning rate is 0.5 
learning_rate = 0.5 
# create instance of neural network 
n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)
```

...

（因为是直接调用库就不打算看下去了）

### 参考文献

全篇整理总结自《python神经网络编程》